{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f2bb68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENETIC ALGORITHM FEATURE SELECTION - DUAL DATASET COMPARISON\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Complete GA-based Feature Selection with Dual Dataset Comparison\n",
    "# ===================================================================\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from math import pi\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENETIC ALGORITHM FEATURE SELECTION - DUAL DATASET COMPARISON\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38aeacb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Loading and preprocessing datasets...\n",
      "Original dataset shape: (149, 33050)\n",
      "P-values file shape: (33146, 3)\n",
      "Features in dataset: 33048\n",
      "Features missing in p-value file: 2\n",
      "Extra features in p-value file: 0\n",
      "Weight statistics - Min: 0.0100, Max: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 1: DATA LOADING AND PREPROCESSING\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n1. Loading and preprocessing datasets...\")\n",
    "\n",
    "# Load original dataset\n",
    "dataset_path = '3_merged_data3.txt'\n",
    "data = pd.read_csv(dataset_path, sep='\\t')\n",
    "print(f\"Original dataset shape: {data.shape}\")\n",
    "\n",
    "# Load p-values file for weighting\n",
    "pval_path = '3_transposed_headers_with_scores.txt'\n",
    "pvals = pd.read_csv(pval_path, sep='\\t')\n",
    "print(f\"P-values file shape: {pvals.shape}\")\n",
    "\n",
    "# Feature analysis\n",
    "target_col = 'avg7_calingiri'\n",
    "feature_cols = [col for col in data.columns if col != target_col and col != 'ID']\n",
    "\n",
    "dataset_features = set(data.columns)\n",
    "pval_features = set(pvals['isoform'])\n",
    "\n",
    "missing_in_pval = dataset_features - pval_features\n",
    "extra_in_pval = pval_features - dataset_features\n",
    "\n",
    "print(f\"Features in dataset: {len(feature_cols)}\")\n",
    "print(f\"Features missing in p-value file: {len(missing_in_pval)}\")\n",
    "print(f\"Extra features in p-value file: {len(extra_in_pval)}\")\n",
    "\n",
    "# Extract features and target\n",
    "X = data[feature_cols]\n",
    "y = data[target_col]\n",
    "\n",
    "# Create weight mapping from p-values\n",
    "pval_map = pvals.set_index('isoform')['p-value_lowest'].to_dict()\n",
    "\n",
    "weights = {}\n",
    "for feat in feature_cols:\n",
    "    p = pval_map.get(feat, None)\n",
    "    if p is None:\n",
    "        weights[feat] = 1.0\n",
    "    else:\n",
    "        weights[feat] = 1.0 / (p + 1e-8)\n",
    "\n",
    "# Step 1: Clamp weights to avoid extremes [0.01, 100]\n",
    "weights_clamped = {\n",
    "    feat: np.clip(weight, 1e-2, 100)\n",
    "    for feat, weight in weights.items()\n",
    "}\n",
    "\n",
    "# Step 2: Normalize weights to range [0.01, 1]\n",
    "clamped_vals = np.array(list(weights_clamped.values()))\n",
    "min_val, max_val = clamped_vals.min(), clamped_vals.max()\n",
    "weights_norm = {\n",
    "    feat: ((val - min_val) / (max_val - min_val)) * (1 - 0.01) + 0.01\n",
    "    for feat, val in weights_clamped.items()\n",
    "}\n",
    "\n",
    "print(f\"Weight statistics - Min: {min(weights_norm.values()):.4f}, Max: {max(weights_norm.values()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebf73ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Preparing original and weighted datasets...\n",
      "Original training set shape: (119, 33048)\n",
      "Weighted training set shape: (119, 33048)\n",
      "\n",
      "3. Applying variance threshold filtering...\n",
      "Original dataset features after filtering: 30033\n",
      "Weighted dataset features after filtering: 29946\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 2: DATASET PREPARATION\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n2. Preparing original and weighted datasets...\")\n",
    "\n",
    "# Train/test split for original data\n",
    "X_train_orig, X_test_orig, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create weighted dataset\n",
    "weights_series = pd.Series(weights_norm)\n",
    "X_train_weighted = X_train_orig * weights_series\n",
    "X_test_weighted = X_test_orig * weights_series\n",
    "\n",
    "print(f\"Original training set shape: {X_train_orig.shape}\")\n",
    "print(f\"Weighted training set shape: {X_train_weighted.shape}\")\n",
    "\n",
    "# Apply variance threshold filtering to remove low-variance features\n",
    "print(\"\\n3. Applying variance threshold filtering...\")\n",
    "\n",
    "# For original dataset\n",
    "selector_orig = VarianceThreshold(threshold=1e-5)\n",
    "X_train_orig_filtered = selector_orig.fit_transform(X_train_orig)\n",
    "X_test_orig_filtered = selector_orig.transform(X_test_orig)\n",
    "\n",
    "mask_orig = selector_orig.get_support()\n",
    "filtered_features_orig = [name for name, keep in zip(feature_cols, mask_orig) if keep]\n",
    "\n",
    "# For weighted dataset\n",
    "selector_weighted = VarianceThreshold(threshold=1e-5)\n",
    "X_train_weighted_filtered = selector_weighted.fit_transform(X_train_weighted)\n",
    "X_test_weighted_filtered = selector_weighted.transform(X_test_weighted)\n",
    "\n",
    "mask_weighted = selector_weighted.get_support()\n",
    "filtered_features_weighted = [name for name, keep in zip(feature_cols, mask_weighted) if keep]\n",
    "\n",
    "print(f\"Original dataset features after filtering: {len(filtered_features_orig)}\")\n",
    "print(f\"Weighted dataset features after filtering: {len(filtered_features_weighted)}\")\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_orig_df = pd.DataFrame(X_train_orig_filtered, columns=filtered_features_orig)\n",
    "X_test_orig_df = pd.DataFrame(X_test_orig_filtered, columns=filtered_features_orig)\n",
    "X_train_weighted_df = pd.DataFrame(X_train_weighted_filtered, columns=filtered_features_weighted)\n",
    "X_test_weighted_df = pd.DataFrame(X_test_weighted_filtered, columns=filtered_features_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "805daf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Setting up Genetic Algorithm parameters...\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 3: GENETIC ALGORITHM SETUP\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n4. Setting up Genetic Algorithm parameters...\")\n",
    "\n",
    "# Models to test\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'LassoRegression': Lasso(random_state=42),\n",
    "    #'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
    "    #'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=2),\n",
    "    #'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "    #'XGBoost': XGBRegressor(random_state=42, n_jobs=2, verbosity=0),\n",
    "    #'LightGBM': LGBMRegressor(random_state=42, n_jobs=2, verbose=-1),\n",
    "    #'CatBoost': CatBoostRegressor(random_state=42, verbose=False)\n",
    "}\n",
    "\n",
    "# GA Parameters\n",
    "k_values = [3, 5, 10]\n",
    "population_size = 25\n",
    "mutation_rate = 0.15\n",
    "num_generations = 20\n",
    "\n",
    "# GA Functions\n",
    "def initialize_population(num_features, pop_size):\n",
    "    \"\"\"Initialize population ensuring at least one feature is selected\"\"\"\n",
    "    np.random.seed(42)\n",
    "    population = []\n",
    "    for _ in range(pop_size):\n",
    "        features = np.random.randint(2, size=num_features)\n",
    "        if np.sum(features) == 0:\n",
    "            features[np.random.randint(num_features)] = 1\n",
    "        population.append(features)\n",
    "    return population\n",
    "\n",
    "def fitness_function(individual, model, X_train, y_train, kfold):\n",
    "    \"\"\"Evaluate fitness using cross-validation\"\"\"\n",
    "    selected_cols = np.where(individual == 1)[0]\n",
    "    \n",
    "    if len(selected_cols) == 0:\n",
    "        return np.inf\n",
    "    \n",
    "    X_selected = X_train.iloc[:, selected_cols]\n",
    "    \n",
    "    try:\n",
    "        scores = -cross_val_score(model, X_selected, y_train, cv=kfold,\n",
    "                                 scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        return scores.mean()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fitness function: {e}\")\n",
    "        return np.inf\n",
    "\n",
    "def selection(population, scores):\n",
    "    \"\"\"Tournament selection\"\"\"\n",
    "    selected = []\n",
    "    for _ in range(len(population)):\n",
    "        i1, i2 = random.sample(range(len(population)), 2)\n",
    "        selected.append(population[i1] if scores[i1] < scores[i2] else population[i2])\n",
    "    return selected\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    \"\"\"Single-point crossover\"\"\"\n",
    "    if len(parent1) == 1:\n",
    "        return parent1.copy(), parent2.copy()\n",
    "    \n",
    "    split_point = random.randint(1, len(parent1) - 1)\n",
    "    child1 = np.concatenate((parent1[:split_point], parent2[split_point:]))\n",
    "    child2 = np.concatenate((parent2[:split_point], parent1[split_point:]))\n",
    "    return child1, child2\n",
    "\n",
    "def mutate(individual, mutation_rate):\n",
    "    \"\"\"Bit-flip mutation with at least one feature guarantee\"\"\"\n",
    "    individual_copy = individual.copy()\n",
    "    for i in range(len(individual_copy)):\n",
    "        if random.random() < mutation_rate:\n",
    "            individual_copy[i] = 1 - individual_copy[i]\n",
    "    \n",
    "    # Ensure at least one feature is selected\n",
    "    if np.sum(individual_copy) == 0:\n",
    "        individual_copy[np.random.randint(len(individual_copy))] = 1\n",
    "    \n",
    "    return individual_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad22111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Running Genetic Algorithm on both datasets...\n",
      "\\n============================================================\n",
      "PROCESSING ORIGINAL DATASET\n",
      "============================================================\n",
      "Training set shape: (119, 30033)\n",
      "Test set shape: (30, 30033)\n",
      "\\nRunning LinearRegression with 3-fold CV on Original dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LinearRegression_k3_Original: 100%|██████████| 20/20 [02:53<00:00,  8.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LinearRegression (k=3): MSE=2.0361, R²=-0.4778, Features=14857\n",
      "\\nRunning LinearRegression with 5-fold CV on Original dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LinearRegression_k5_Original: 100%|██████████| 20/20 [03:34<00:00, 10.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LinearRegression (k=5): MSE=1.9600, R²=-0.4226, Features=15037\n",
      "\\nRunning LinearRegression with 10-fold CV on Original dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LinearRegression_k10_Original:  50%|█████     | 10/20 [03:27<03:33, 21.37s/it]"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# STEP 4: GENETIC ALGORITHM EXECUTION\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n5. Running Genetic Algorithm on both datasets...\")\n",
    "\n",
    "# Storage for results\n",
    "all_results = []\n",
    "save_dir = \"ga_comparison_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Dataset configurations\n",
    "datasets = {\n",
    "    'Original': {\n",
    "        'X_train': X_train_orig_df,\n",
    "        'X_test': X_test_orig_df,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    },\n",
    "    'Weighted': {\n",
    "        'X_train': X_train_weighted_df,\n",
    "        'X_test': X_test_weighted_df,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test\n",
    "    }\n",
    "}\n",
    "\n",
    "# Main GA execution loop\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(f\"\\\\n{'='*60}\")\n",
    "    print(f\"PROCESSING {dataset_name.upper()} DATASET\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X_train_current = dataset['X_train']\n",
    "    X_test_current = dataset['X_test']\n",
    "    y_train_current = dataset['y_train']\n",
    "    y_test_current = dataset['y_test']\n",
    "    \n",
    "    print(f\"Training set shape: {X_train_current.shape}\")\n",
    "    print(f\"Test set shape: {X_test_current.shape}\")\n",
    "    \n",
    "    for model_name, base_model in models.items():\n",
    "        for k in k_values:\n",
    "            print(f\"\\\\nRunning {model_name} with {k}-fold CV on {dataset_name} dataset...\")\n",
    "            \n",
    "            # Create fresh model instance\n",
    "            if model_name == 'LinearRegression':\n",
    "                model = LinearRegression()\n",
    "            elif model_name == 'LassoRegression':\n",
    "                model = Lasso(random_state=42)\n",
    "            elif model_name == 'DecisionTree':\n",
    "                model = DecisionTreeRegressor(random_state=42)\n",
    "            elif model_name == 'RandomForest':\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=2)\n",
    "            elif model_name == 'GradientBoosting':\n",
    "                model = GradientBoostingRegressor(random_state=42)\n",
    "            elif model_name == 'XGBoost':\n",
    "                model = XGBRegressor(random_state=42, n_jobs=2, verbosity=0)\n",
    "            elif model_name == 'LightGBM':\n",
    "                model = LGBMRegressor(random_state=42, n_jobs=2, verbose=-1)\n",
    "            elif model_name == 'CatBoost':\n",
    "                model = CatBoostRegressor(random_state=42, verbose=False)\n",
    "            \n",
    "            # Initialize GA\n",
    "            kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "            population = initialize_population(X_train_current.shape[1], population_size)\n",
    "            \n",
    "            generation_progress = []\n",
    "            best_fitness_history = []\n",
    "            \n",
    "            # Evolution loop\n",
    "            for generation in tqdm(range(num_generations), \n",
    "                                 desc=f\"{model_name}_k{k}_{dataset_name}\"):\n",
    "                \n",
    "                # Evaluate fitness for all individuals\n",
    "                fitness_scores = []\n",
    "                for individual in population:\n",
    "                    fitness = fitness_function(individual, model, X_train_current, \n",
    "                                             y_train_current, kfold)\n",
    "                    fitness_scores.append(fitness)\n",
    "                \n",
    "                # Track best fitness\n",
    "                best_fitness = np.min(fitness_scores)\n",
    "                generation_progress.append(best_fitness)\n",
    "                best_fitness_history.append(best_fitness)\n",
    "                \n",
    "                # Selection\n",
    "                sorted_indices = np.argsort(fitness_scores)\n",
    "                elite_size = population_size // 4\n",
    "                elite_population = [population[i] for i in sorted_indices[:elite_size]]\n",
    "                \n",
    "                # Create new population\n",
    "                new_population = elite_population.copy()\n",
    "                \n",
    "                while len(new_population) < population_size:\n",
    "                    # Select parents\n",
    "                    parent1, parent2 = random.sample(elite_population, 2)\n",
    "                    \n",
    "                    # Crossover\n",
    "                    child1, child2 = crossover(parent1, parent2)\n",
    "                    \n",
    "                    # Mutation\n",
    "                    child1 = mutate(child1, mutation_rate)\n",
    "                    child2 = mutate(child2, mutation_rate)\n",
    "                    \n",
    "                    new_population.extend([child1, child2])\n",
    "                \n",
    "                population = new_population[:population_size]\n",
    "            \n",
    "            # Get best solution\n",
    "            final_fitness = [fitness_function(ind, model, X_train_current, \n",
    "                                            y_train_current, kfold) for ind in population]\n",
    "            best_individual = population[np.argmin(final_fitness)]\n",
    "            selected_features = np.where(best_individual == 1)[0]\n",
    "            \n",
    "            if len(selected_features) == 0:\n",
    "                print(f\"Warning: No features selected for {model_name}_k{k}_{dataset_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Train final model\n",
    "            X_train_selected = X_train_current.iloc[:, selected_features]\n",
    "            X_test_selected = X_test_current.iloc[:, selected_features]\n",
    "            \n",
    "            model.fit(X_train_selected, y_train_current)\n",
    "            y_pred = model.predict(X_test_selected)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            test_mse = mean_squared_error(y_test_current, y_pred)\n",
    "            test_mae = mean_absolute_error(y_test_current, y_pred)\n",
    "            test_r2 = r2_score(y_test_current, y_pred)\n",
    "            cv_mse = np.min(final_fitness)\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'Dataset': dataset_name,\n",
    "                'Model': model_name,\n",
    "                'K_Folds': k,\n",
    "                'CV_MSE': cv_mse,\n",
    "                'Test_MSE': test_mse,\n",
    "                'Test_MAE': test_mae,\n",
    "                'Test_R2': test_r2,\n",
    "                'Num_Features': len(selected_features),\n",
    "                'Total_Features': X_train_current.shape[1],\n",
    "                'Feature_Ratio': len(selected_features) / X_train_current.shape[1],\n",
    "                'Selected_Features_Idx': selected_features.tolist(),\n",
    "                'Selected_Features_Names': [X_train_current.columns[i] for i in selected_features],\n",
    "                'Generation_Progress': generation_progress,\n",
    "                'Convergence_Generation': len(generation_progress) - np.argmin(generation_progress[::-1]) - 1\n",
    "            }\n",
    "            \n",
    "            all_results.append(result)\n",
    "            \n",
    "            # Save model\n",
    "            model_filename = f\"{dataset_name}_{model_name}_k{k}_ga_model.joblib\"\n",
    "            model_path = os.path.join(save_dir, model_filename)\n",
    "            \n",
    "            joblib.dump({\n",
    "                'model': model,\n",
    "                'selected_features_idx': selected_features,\n",
    "                'selected_features_names': result['Selected_Features_Names'],\n",
    "                'dataset_type': dataset_name,\n",
    "                'model_name': model_name,\n",
    "                'k_folds': k,\n",
    "                'performance_metrics': {\n",
    "                    'cv_mse': cv_mse,\n",
    "                    'test_mse': test_mse,\n",
    "                    'test_mae': test_mae,\n",
    "                    'test_r2': test_r2\n",
    "                },\n",
    "                'ga_progress': generation_progress\n",
    "            }, model_path)\n",
    "            \n",
    "            print(f\"✓ {model_name} (k={k}): MSE={test_mse:.4f}, R²={test_r2:.4f}, Features={len(selected_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f13e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# STEP 5: RESULTS ANALYSIS AND COMPARISON\n",
    "# ===================================================================\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(\"RESULTS ANALYSIS AND COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save comprehensive results\n",
    "results_csv_path = 'ga_dual_dataset_comprehensive_results.csv'\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "print(f\"\\\\nComprehensive results saved to: {results_csv_path}\")\n",
    "\n",
    "# Statistical Summary\n",
    "print(\"\\\\n6. Statistical Summary:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "summary_stats = results_df.groupby('Dataset').agg({\n",
    "    'Test_MSE': ['mean', 'std', 'min', 'max'],\n",
    "    'Test_R2': ['mean', 'std', 'min', 'max'],\n",
    "    'Test_MAE': ['mean', 'std', 'min', 'max'],\n",
    "    'Num_Features': ['mean', 'std', 'min', 'max'],\n",
    "    'Feature_Ratio': ['mean', 'std']\n",
    "}).round(4)\n",
    "\n",
    "print(summary_stats)\n",
    "\n",
    "# Best models per dataset\n",
    "print(\"\\\\n7. Best Models by Dataset:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for dataset in ['Original', 'Weighted']:\n",
    "    dataset_results = results_df[results_df['Dataset'] == dataset]\n",
    "    best_mse_idx = dataset_results['Test_MSE'].idxmin()\n",
    "    best_r2_idx = dataset_results['Test_R2'].idxmax()\n",
    "    \n",
    "    print(f\"\\\\n{dataset} Dataset:\")\n",
    "    print(f\"Best MSE: {dataset_results.loc[best_mse_idx, 'Model']} (k={dataset_results.loc[best_mse_idx, 'K_Folds']}) - MSE: {dataset_results.loc[best_mse_idx, 'Test_MSE']:.4f}\")\n",
    "    print(f\"Best R²:  {dataset_results.loc[best_r2_idx, 'Model']} (k={dataset_results.loc[best_r2_idx, 'K_Folds']}) - R²: {dataset_results.loc[best_r2_idx, 'Test_R2']:.4f}\")\n",
    "\n",
    "# Model comparison\n",
    "print(\"\\\\n8. Model Performance Comparison:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "model_comparison = results_df.groupby(['Model', 'Dataset']).agg({\n",
    "    'Test_MSE': 'mean',\n",
    "    'Test_R2': 'mean',\n",
    "    'Num_Features': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(model_comparison)\n",
    "\n",
    "# Dataset winner analysis\n",
    "print(\"\\\\n9. Overall Dataset Performance:\")\n",
    "print(\"-\" * 38)\n",
    "\n",
    "orig_avg_mse = results_df[results_df['Dataset'] == 'Original']['Test_MSE'].mean()\n",
    "weighted_avg_mse = results_df[results_df['Dataset'] == 'Weighted']['Test_MSE'].mean()\n",
    "orig_avg_r2 = results_df[results_df['Dataset'] == 'Original']['Test_R2'].mean()\n",
    "weighted_avg_r2 = results_df[results_df['Dataset'] == 'Weighted']['Test_R2'].mean()\n",
    "\n",
    "print(f\"Original Dataset  - Avg MSE: {orig_avg_mse:.4f}, Avg R²: {orig_avg_r2:.4f}\")\n",
    "print(f\"Weighted Dataset  - Avg MSE: {weighted_avg_mse:.4f}, Avg R²: {weighted_avg_r2:.4f}\")\n",
    "print(f\"\\\\nWinner by MSE: {'Weighted' if weighted_avg_mse < orig_avg_mse else 'Original'} Dataset\")\n",
    "print(f\"Winner by R²:  {'Weighted' if weighted_avg_r2 > orig_avg_r2 else 'Original'} Dataset\")\n",
    "print(f\"MSE Improvement: {abs(orig_avg_mse - weighted_avg_mse):.4f}\")\n",
    "print(f\"R² Improvement:  {abs(weighted_avg_r2 - orig_avg_r2):.4f}\")\n",
    "\n",
    "print(f\"\\\\n{'='*80}\")\n",
    "print(\"GENETIC ALGORITHM EXECUTION COMPLETED SUCCESSFULLY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total experiments: {len(results_df)}\")\n",
    "print(f\"Models saved in: {save_dir}\")\n",
    "print(f\"Results saved to: {results_csv_path}\")\n",
    "print(\"Ready for visualization phase!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d711f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ce8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# COMPREHENSIVE VISUALIZATION AND ANALYSIS\n",
    "# ===================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import pi\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy import stats\n",
    "\n",
    "# Load results (assuming results_df is available from previous execution)\n",
    "# If running separately, uncomment the line below:\n",
    "# results_df = pd.read_csv('ga_dual_dataset_comprehensive_results.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE VISUALIZATION AND ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# ===================================================================\n",
    "# 1. OVERALL PERFORMANCE COMPARISON\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n1. Creating overall performance comparison plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Overall Performance Comparison: Original vs Weighted Datasets', fontsize=16, fontweight='bold')\n",
    "\n",
    "# MSE Comparison\n",
    "sns.boxplot(data=results_df, x='Dataset', y='Test_MSE', ax=axes[0,0])\n",
    "axes[0,0].set_title('Test MSE Distribution')\n",
    "axes[0,0].set_ylabel('MSE (Lower is Better)')\n",
    "\n",
    "# R² Comparison\n",
    "sns.boxplot(data=results_df, x='Dataset', y='Test_R2', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test R² Distribution')\n",
    "axes[0,1].set_ylabel('R² (Higher is Better)')\n",
    "\n",
    "# MAE Comparison\n",
    "sns.boxplot(data=results_df, x='Dataset', y='Test_MAE', ax=axes[0,2])\n",
    "axes[0,2].set_title('Test MAE Distribution')\n",
    "axes[0,2].set_ylabel('MAE (Lower is Better)')\n",
    "\n",
    "# Feature Usage\n",
    "sns.boxplot(data=results_df, x='Dataset', y='Num_Features', ax=axes[1,0])\n",
    "axes[1,0].set_title('Number of Features Selected')\n",
    "axes[1,0].set_ylabel('Number of Features')\n",
    "\n",
    "# Feature Ratio\n",
    "sns.boxplot(data=results_df, x='Dataset', y='Feature_Ratio', ax=axes[1,1])\n",
    "axes[1,1].set_title('Feature Selection Ratio')\n",
    "axes[1,1].set_ylabel('Ratio of Selected Features')\n",
    "\n",
    "# CV vs Test MSE\n",
    "sns.scatterplot(data=results_df, x='CV_MSE', y='Test_MSE', hue='Dataset', ax=axes[1,2])\n",
    "axes[1,2].set_title('CV MSE vs Test MSE')\n",
    "axes[1,2].set_xlabel('Cross-Validation MSE')\n",
    "axes[1,2].set_ylabel('Test MSE')\n",
    "axes[1,2].plot([0, axes[1,2].get_xlim()[1]], [0, axes[1,2].get_ylim()[1]], 'k--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877fc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 2. MODEL PERFORMANCE ANALYSIS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n2. Creating model performance analysis...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Model Performance Analysis Across Datasets', fontsize=16, fontweight='bold')\n",
    "\n",
    "# MSE by Model and Dataset\n",
    "pivot_mse = results_df.pivot_table(values='Test_MSE', index='Model', columns='Dataset', aggfunc='mean')\n",
    "sns.heatmap(pivot_mse, annot=True, fmt='.4f', cmap='Reds', ax=axes[0,0])\n",
    "axes[0,0].set_title('Average Test MSE by Model and Dataset')\n",
    "\n",
    "# R² by Model and Dataset\n",
    "pivot_r2 = results_df.pivot_table(values='Test_R2', index='Model', columns='Dataset', aggfunc='mean')\n",
    "sns.heatmap(pivot_r2, annot=True, fmt='.4f', cmap='Blues', ax=axes[0,1])\n",
    "axes[0,1].set_title('Average Test R² by Model and Dataset')\n",
    "\n",
    "# Feature Usage by Model\n",
    "sns.barplot(data=results_df, x='Model', y='Num_Features', hue='Dataset', ax=axes[1,0])\n",
    "axes[1,0].set_title('Average Features Selected by Model')\n",
    "axes[1,0].set_xlabel('Model')\n",
    "axes[1,0].set_ylabel('Number of Features')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Performance vs Complexity\n",
    "sns.scatterplot(data=results_df, x='Num_Features', y='Test_R2', \n",
    "                hue='Dataset', style='Model', s=100, ax=axes[1,1])\n",
    "axes[1,1].set_title('Performance vs Model Complexity')\n",
    "axes[1,1].set_xlabel('Number of Features Selected')\n",
    "axes[1,1].set_ylabel('Test R² Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 3. K-FOLD ANALYSIS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n3. Creating K-fold cross-validation analysis...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('K-Fold Cross-Validation Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# MSE by K-Folds\n",
    "sns.lineplot(data=results_df, x='K_Folds', y='Test_MSE', hue='Dataset', \n",
    "             marker='o', markersize=8, ax=axes[0,0])\n",
    "axes[0,0].set_title('Test MSE vs K-Fold Values')\n",
    "axes[0,0].set_xlabel('K-Fold Value')\n",
    "axes[0,0].set_ylabel('Test MSE')\n",
    "\n",
    "# R² by K-Folds\n",
    "sns.lineplot(data=results_df, x='K_Folds', y='Test_R2', hue='Dataset', \n",
    "             marker='o', markersize=8, ax=axes[0,1])\n",
    "axes[0,1].set_title('Test R² vs K-Fold Values')\n",
    "axes[0,1].set_xlabel('K-Fold Value')\n",
    "axes[0,1].set_ylabel('Test R²')\n",
    "\n",
    "# Feature selection by K-Folds\n",
    "sns.lineplot(data=results_df, x='K_Folds', y='Num_Features', hue='Dataset', \n",
    "             marker='o', markersize=8, ax=axes[1,0])\n",
    "axes[1,0].set_title('Feature Selection vs K-Fold Values')\n",
    "axes[1,0].set_xlabel('K-Fold Value')\n",
    "axes[1,0].set_ylabel('Number of Features Selected')\n",
    "\n",
    "# Variance in performance\n",
    "mse_variance = results_df.groupby(['Dataset', 'K_Folds'])['Test_MSE'].var().reset_index()\n",
    "sns.barplot(data=mse_variance, x='K_Folds', y='Test_MSE', hue='Dataset', ax=axes[1,1])\n",
    "axes[1,1].set_title('MSE Variance by K-Fold Values')\n",
    "axes[1,1].set_xlabel('K-Fold Value')\n",
    "axes[1,1].set_ylabel('MSE Variance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 4. GENETIC ALGORITHM EVOLUTION ANALYSIS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n4. Creating GA evolution analysis...\")\n",
    "\n",
    "# Plot evolution for best models from each dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Genetic Algorithm Evolution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Best model evolution from each dataset\n",
    "datasets = ['Original', 'Weighted']\n",
    "colors = ['red', 'blue']\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    dataset_results = results_df[results_df['Dataset'] == dataset]\n",
    "    best_idx = dataset_results['Test_MSE'].idxmin()\n",
    "    best_result = dataset_results.loc[best_idx]\n",
    "    \n",
    "    if 'Generation_Progress' in best_result and best_result['Generation_Progress']:\n",
    "        evolution = eval(best_result['Generation_Progress']) if isinstance(best_result['Generation_Progress'], str) else best_result['Generation_Progress']\n",
    "        axes[i, 0].plot(evolution, color=colors[i], linewidth=2, marker='o', markersize=4)\n",
    "        axes[i, 0].set_title(f'Best Model Evolution - {dataset} Dataset\\n({best_result[\"Model\"]}, k={best_result[\"K_Folds\"]})')\n",
    "        axes[i, 0].set_xlabel('Generation')\n",
    "        axes[i, 0].set_ylabel('Cross-Validation MSE')\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Convergence analysis\n",
    "convergence_data = []\n",
    "for dataset in datasets:\n",
    "    dataset_results = results_df[results_df['Dataset'] == dataset]\n",
    "    for idx, row in dataset_results.iterrows():\n",
    "        if 'Generation_Progress' in row and row['Generation_Progress']:\n",
    "            try:\n",
    "                evolution = eval(row['Generation_Progress']) if isinstance(row['Generation_Progress'], str) else row['Generation_Progress']\n",
    "                if len(evolution) > 1:\n",
    "                    # Calculate convergence metrics\n",
    "                    final_improvement = evolution[0] - evolution[-1]\n",
    "                    convergence_rate = final_improvement / len(evolution)\n",
    "                    \n",
    "                    convergence_data.append({\n",
    "                        'Dataset': dataset,\n",
    "                        'Model': row['Model'],\n",
    "                        'K_Folds': row['K_Folds'],\n",
    "                        'Final_Improvement': final_improvement,\n",
    "                        'Convergence_Rate': convergence_rate,\n",
    "                        'Generations': len(evolution)\n",
    "                    })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "if convergence_data:\n",
    "    convergence_df = pd.DataFrame(convergence_data)\n",
    "    \n",
    "    # Plot convergence rates by dataset\n",
    "    sns.boxplot(data=convergence_df, x='Dataset', y='Convergence_Rate', ax=axes[0,1])\n",
    "    axes[0,1].set_title('GA Convergence Rate Distribution')\n",
    "    axes[0,1].set_ylabel('Convergence Rate (MSE/Generation)')\n",
    "    \n",
    "    # Plot final improvement by model\n",
    "    sns.barplot(data=convergence_df, x='Model', y='Final_Improvement', hue='Dataset', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Final GA Improvement by Model')\n",
    "    axes[1,1].set_xlabel('Model')\n",
    "    axes[1,1].set_ylabel('Final Improvement (MSE)')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af9d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 5. FEATURE IMPORTANCE AND SELECTION ANALYSIS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n5. Creating feature importance analysis...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Feature Selection and Importance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Feature count distribution\n",
    "feature_counts = results_df.groupby(['Dataset', 'Num_Features']).size().reset_index(name='Count')\n",
    "pivot_features = feature_counts.pivot(index='Num_Features', columns='Dataset', values='Count').fillna(0)\n",
    "pivot_features.plot(kind='bar', ax=axes[0,0], color=['orange', 'green'])\n",
    "axes[0,0].set_title('Distribution of Feature Counts')\n",
    "axes[0,0].set_xlabel('Number of Features Selected')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].legend(title='Dataset')\n",
    "\n",
    "# Feature ratio vs performance\n",
    "sns.scatterplot(data=results_df, x='Feature_Ratio', y='Test_R2', \n",
    "                hue='Dataset', style='Model', s=100, ax=axes[0,1])\n",
    "axes[0,1].set_title('Feature Ratio vs Performance')\n",
    "axes[0,1].set_xlabel('Feature Selection Ratio')\n",
    "axes[0,1].set_ylabel('Test R² Score')\n",
    "\n",
    "# Model efficiency (Performance per feature)\n",
    "results_df['Efficiency'] = results_df['Test_R2'] / results_df['Num_Features']\n",
    "sns.boxplot(data=results_df, x='Model', y='Efficiency', hue='Dataset', ax=axes[1,0])\n",
    "axes[1,0].set_title('Model Efficiency (R²/Feature)')\n",
    "axes[1,0].set_xlabel('Model')\n",
    "axes[1,0].set_ylabel('Efficiency Score')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Feature selection consistency\n",
    "feature_consistency = results_df.groupby(['Model', 'Dataset'])['Num_Features'].std().reset_index()\n",
    "feature_consistency.columns = ['Model', 'Dataset', 'Feature_Std']\n",
    "pivot_consistency = feature_consistency.pivot(index='Model', columns='Dataset', values='Feature_Std')\n",
    "sns.heatmap(pivot_consistency, annot=True, fmt='.2f', cmap='YlOrRd', ax=axes[1,1])\n",
    "axes[1,1].set_title('Feature Selection Consistency\\n(Standard Deviation)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb379cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 6. STATISTICAL ANALYSIS AND SIGNIFICANCE TESTING\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n6. Performing statistical analysis...\")\n",
    "\n",
    "# Perform statistical tests\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICAL SIGNIFICANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# T-test for MSE differences between datasets\n",
    "original_mse = results_df[results_df['Dataset'] == 'Original']['Test_MSE']\n",
    "weighted_mse = results_df[results_df['Dataset'] == 'Weighted']['Test_MSE']\n",
    "\n",
    "t_stat_mse, p_val_mse = stats.ttest_ind(original_mse, weighted_mse)\n",
    "print(f\"\\nMSE Comparison (Original vs Weighted):\")\n",
    "print(f\"  Original MSE - Mean: {original_mse.mean():.6f}, Std: {original_mse.std():.6f}\")\n",
    "print(f\"  Weighted MSE - Mean: {weighted_mse.mean():.6f}, Std: {weighted_mse.std():.6f}\")\n",
    "print(f\"  T-statistic: {t_stat_mse:.4f}\")\n",
    "print(f\"  P-value: {p_val_mse:.6f}\")\n",
    "print(f\"  Significant difference: {'Yes' if p_val_mse < 0.05 else 'No'}\")\n",
    "\n",
    "# T-test for R² differences\n",
    "original_r2 = results_df[results_df['Dataset'] == 'Original']['Test_R2']\n",
    "weighted_r2 = results_df[results_df['Dataset'] == 'Weighted']['Test_R2']\n",
    "\n",
    "t_stat_r2, p_val_r2 = stats.ttest_ind(original_r2, weighted_r2)\n",
    "print(f\"\\nR² Comparison (Original vs Weighted):\")\n",
    "print(f\"  Original R² - Mean: {original_r2.mean():.6f}, Std: {original_r2.std():.6f}\")\n",
    "print(f\"  Weighted R² - Mean: {weighted_r2.mean():.6f}, Std: {weighted_r2.std():.6f}\")\n",
    "print(f\"  T-statistic: {t_stat_r2:.4f}\")\n",
    "print(f\"  P-value: {p_val_r2:.6f}\")\n",
    "print(f\"  Significant difference: {'Yes' if p_val_r2 < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c32653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 7. PERFORMANCE SUMMARY AND RECOMMENDATIONS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Best performing configurations\n",
    "print(\"\\n7. Best Performing Configurations:\")\n",
    "\n",
    "for dataset in ['Original', 'Weighted']:\n",
    "    dataset_results = results_df[results_df['Dataset'] == dataset]\n",
    "    best_mse_idx = dataset_results['Test_MSE'].idxmin()\n",
    "    best_r2_idx = dataset_results['Test_R2'].idxmax()\n",
    "    \n",
    "    print(f\"\\n{dataset} Dataset:\")\n",
    "    print(f\"  Best MSE: {dataset_results.loc[best_mse_idx, 'Test_MSE']:.6f}\")\n",
    "    print(f\"    Model: {dataset_results.loc[best_mse_idx, 'Model']}\")\n",
    "    print(f\"    K-Folds: {dataset_results.loc[best_mse_idx, 'K_Folds']}\")\n",
    "    print(f\"    Features: {dataset_results.loc[best_mse_idx, 'Num_Features']}\")\n",
    "    \n",
    "    print(f\"  Best R²: {dataset_results.loc[best_r2_idx, 'Test_R2']:.6f}\")\n",
    "    print(f\"    Model: {dataset_results.loc[best_r2_idx, 'Model']}\")\n",
    "    print(f\"    K-Folds: {dataset_results.loc[best_r2_idx, 'K_Folds']}\")\n",
    "    print(f\"    Features: {dataset_results.loc[best_r2_idx, 'Num_Features']}\")\n",
    "\n",
    "# Model ranking by average performance\n",
    "print(\"\\n8. Model Rankings by Average Performance:\")\n",
    "\n",
    "model_performance = results_df.groupby(['Model', 'Dataset']).agg({\n",
    "    'Test_MSE': ['mean', 'std'],\n",
    "    'Test_R2': ['mean', 'std'],\n",
    "    'Num_Features': 'mean'\n",
    "}).round(6)\n",
    "\n",
    "print(\"\\nAverage Test MSE by Model and Dataset:\")\n",
    "print(model_performance['Test_MSE'])\n",
    "\n",
    "print(\"\\nAverage Test R² by Model and Dataset:\")\n",
    "print(model_performance['Test_R2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3edab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 8. FINAL VISUALIZATION SUMMARY\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n9. Creating final summary visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Final Performance Summary: GA Feature Selection Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Overall winner comparison\n",
    "winners_data = []\n",
    "for dataset in ['Original', 'Weighted']:\n",
    "    dataset_results = results_df[results_df['Dataset'] == dataset]\n",
    "    best_idx = dataset_results['Test_MSE'].idxmin()\n",
    "    best_result = dataset_results.loc[best_idx]\n",
    "    winners_data.append({\n",
    "        'Dataset': dataset,\n",
    "        'Best_MSE': best_result['Test_MSE'],\n",
    "        'Best_R2': best_result['Test_R2'],\n",
    "        'Best_Model': best_result['Model'],\n",
    "        'Features_Used': best_result['Num_Features']\n",
    "    })\n",
    "\n",
    "winners_df = pd.DataFrame(winners_data)\n",
    "\n",
    "# Best MSE comparison\n",
    "bars1 = axes[0,0].bar(winners_df['Dataset'], winners_df['Best_MSE'], \n",
    "                      color=['red', 'blue'], alpha=0.7)\n",
    "axes[0,0].set_title('Best MSE Achieved by Dataset')\n",
    "axes[0,0].set_ylabel('Test MSE (Lower is Better)')\n",
    "for i, bar in enumerate(bars1):\n",
    "    height = bar.get_height()\n",
    "    axes[0,0].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                   f'{height:.6f}', ha='center', va='bottom')\n",
    "\n",
    "# Best R² comparison\n",
    "bars2 = axes[0,1].bar(winners_df['Dataset'], winners_df['Best_R2'], \n",
    "                      color=['red', 'blue'], alpha=0.7)\n",
    "axes[0,1].set_title('Best R² Achieved by Dataset')\n",
    "axes[0,1].set_ylabel('Test R² (Higher is Better)')\n",
    "for i, bar in enumerate(bars2):\n",
    "    height = bar.get_height()\n",
    "    axes[0,1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                   f'{height:.6f}', ha='center', va='bottom')\n",
    "\n",
    "# Feature usage comparison\n",
    "bars3 = axes[1,0].bar(winners_df['Dataset'], winners_df['Features_Used'], \n",
    "                      color=['red', 'blue'], alpha=0.7)\n",
    "axes[1,0].set_title('Features Used by Best Models')\n",
    "axes[1,0].set_ylabel('Number of Features')\n",
    "for i, bar in enumerate(bars3):\n",
    "    height = bar.get_height()\n",
    "    axes[1,0].text(bar.get_x() + bar.get_width()/2., height + height*0.05,\n",
    "                   f'{int(height)}', ha='center', va='bottom')\n",
    "\n",
    "# Performance improvement\n",
    "if len(winners_data) >= 2:\n",
    "    mse_improvement = ((winners_data[0]['Best_MSE'] - winners_data[1]['Best_MSE']) / \n",
    "                       winners_data[0]['Best_MSE']) * 100\n",
    "    r2_improvement = ((winners_data[1]['Best_R2'] - winners_data[0]['Best_R2']) / \n",
    "                      winners_data[0]['Best_R2']) * 100\n",
    "    \n",
    "    improvements = ['MSE Reduction (%)', 'R² Improvement (%)']\n",
    "    values = [abs(mse_improvement), r2_improvement]\n",
    "    colors_imp = ['green' if v > 0 else 'red' for v in [mse_improvement, r2_improvement]]\n",
    "    \n",
    "    bars4 = axes[1,1].bar(improvements, values, color=colors_imp, alpha=0.7)\n",
    "    axes[1,1].set_title('Weighted Dataset Improvement over Original')\n",
    "    axes[1,1].set_ylabel('Improvement (%)')\n",
    "    axes[1,1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    for i, bar in enumerate(bars4):\n",
    "        height = bar.get_height()\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2., height + height*0.02,\n",
    "                       f'{height:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837195ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 9. FINAL RECOMMENDATIONS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nBased on the comprehensive analysis:\")\n",
    "\n",
    "# Determine which dataset performed better\n",
    "if len(winners_data) >= 2:\n",
    "    if winners_data[1]['Best_MSE'] < winners_data[0]['Best_MSE']:\n",
    "        print(\"✓ The WEIGHTED dataset shows superior performance overall\")\n",
    "        print(f\"  - Lower MSE: {winners_data[1]['Best_MSE']:.6f} vs {winners_data[0]['Best_MSE']:.6f}\")\n",
    "        print(f\"  - Higher R²: {winners_data[1]['Best_R2']:.6f} vs {winners_data[0]['Best_R2']:.6f}\")\n",
    "    else:\n",
    "        print(\"✓ The ORIGINAL dataset shows superior performance overall\")\n",
    "        print(f\"  - Lower MSE: {winners_data[0]['Best_MSE']:.6f} vs {winners_data[1]['Best_MSE']:.6f}\")\n",
    "        print(f\"  - Higher R²: {winners_data[0]['Best_R2']:.6f} vs {winners_data[1]['Best_R2']:.6f}\")\n",
    "\n",
    "# Best model recommendation\n",
    "best_overall_idx = results_df['Test_MSE'].idxmin()\n",
    "best_overall = results_df.loc[best_overall_idx]\n",
    "\n",
    "print(f\"\\n✓ RECOMMENDED CONFIGURATION:\")\n",
    "print(f\"  - Dataset: {best_overall['Dataset']}\")\n",
    "print(f\"  - Model: {best_overall['Model']}\")\n",
    "print(f\"  - K-Folds: {best_overall['K_Folds']}\")\n",
    "print(f\"  - Features Selected: {best_overall['Num_Features']}\")\n",
    "print(f\"  - Test MSE: {best_overall['Test_MSE']:.6f}\")\n",
    "print(f\"  - Test R²: {best_overall['Test_R2']:.6f}\")\n",
    "\n",
    "print(f\"\\n✓ Key Insights:\")\n",
    "print(f\"  - Feature selection reduced dimensionality effectively\")\n",
    "print(f\"  - Genetic algorithm successfully optimized feature subsets\")\n",
    "print(f\"  - Cross-validation provided robust performance estimates\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
