{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "456fdbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ba48d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (adjust path & delimiter accordingly)\n",
    "dataset_path = '3_merged_data3.txt'\n",
    "data = pd.read_csv(dataset_path, sep='\\t')  # or sep=',' if CSV\n",
    "\n",
    "# Load p-values file\n",
    "pval_path = '3_transposed_headers_with_scores.txt'\n",
    "pvals = pd.read_csv(pval_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a30669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in dataset not in p-value file: 2\n",
      "Missing features:\n",
      " {'avg7_calingiri', 'ID'}\n",
      "Extra features in p-value file not in dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Get list of features from dataset columns\n",
    "dataset_features = set(data.columns)\n",
    "\n",
    "# Get list of features from p-value file\n",
    "pval_features = set(pvals['isoform'])\n",
    "\n",
    "# Find missing and extra features\n",
    "missing_in_pval = dataset_features - pval_features\n",
    "extra_in_pval = pval_features - dataset_features\n",
    "\n",
    "# Print results\n",
    "print(f\"Features in dataset not in p-value file: {len(missing_in_pval)}\")\n",
    "if missing_in_pval:\n",
    "    print(\"Missing features:\\n\", missing_in_pval)\n",
    "\n",
    "print(f\"Extra features in p-value file not in dataset: {len(extra_in_pval)}\")\n",
    "if extra_in_pval:\n",
    "    print(\"Extra features:\\n\", extra_in_pval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791e8aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 33048)\n"
     ]
    }
   ],
   "source": [
    "# Extract features and target (assuming 'avg7_calingiri' is target)\n",
    "target_col = 'avg7_calingiri'\n",
    "feature_cols = [col for col in data.columns if col != target_col and col != 'ID']\n",
    "\n",
    "X = data[feature_cols]\n",
    "y = data[target_col]\n",
    "\n",
    "# Map p-values by feature\n",
    "pval_map = pvals.set_index('isoform')['p-value_lowest'].to_dict()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f372d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "for feat in feature_cols:\n",
    "    p = pval_map.get(feat, None)\n",
    "    if p is None:\n",
    "        weights[feat] = 1.0\n",
    "    else:\n",
    "        weights[feat] = 1.0 / (p + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d4b9f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb1fafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clamp weights to avoid extremes [0.01, 100]\n",
    "weights_clamped = {\n",
    "    feat: np.clip(weight, 1e-2, 100)\n",
    "    for feat, weight in weights.items()\n",
    "}\n",
    "\n",
    "# Step 2: Normalize weights to range [0.01, 1]\n",
    "clamped_vals = np.array(list(weights_clamped.values()))\n",
    "min_val, max_val = clamped_vals.min(), clamped_vals.max()\n",
    "\n",
    "weights_norm = {\n",
    "    feat: ((val - min_val) / (max_val - min_val)) * (1 - 0.01) + 0.01\n",
    "    for feat, val in weights_clamped.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ee9e808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(weights_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3159d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b62aa3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119, 33048)\n",
      "New weighted features stats: 0.0 4.0 0.20485378840529941\n"
     ]
    }
   ],
   "source": [
    "weights_series = pd.Series(weights_norm)\n",
    "\n",
    "# Now multiply features by these  weights\n",
    "X_train_weighted = X_train * weights_series\n",
    "X_test_weighted = X_test * weights_series\n",
    "print(X_train_weighted.shape)\n",
    "print(\"New weighted features stats:\",\n",
    "      X_train_weighted.min().min(),\n",
    "      X_train_weighted.max().max(),\n",
    "      X_train_weighted.std().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "247fe511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature variance stats: 0.0 1.260221735753125\n",
      "Target variance: 1.6101913424193204\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature variance stats:\", np.var(X_train_weighted, axis=0).min(), np.var(X_train_weighted, axis=0).max())\n",
    "print(\"Target variance:\", np.var(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "680f9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Step 1: Init the selector\n",
    "selector = VarianceThreshold(threshold=1e-5)\n",
    "\n",
    "# Step 2: Fit only on TRAIN\n",
    "X_train_filtered = selector.fit_transform(X_train_weighted)\n",
    "\n",
    "# Step 3: Use .transform() on VAL and TEST\n",
    "X_test_filtered = selector.transform(X_test_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a8101dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=100, random_state=42),\n",
    "    'CatBoost': CatBoostRegressor(iterations=100, random_seed=42, verbose=False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c28d8758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119, 29946)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e0ac31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 07:42:54,819 - INFO - Starting training for 3 models...\n",
      "Models:   0%|          | 0/3 [00:00<?, ?model/s]2025-06-04 07:42:54,822 - INFO - Training XGBoost...\n",
      "c:\\Users\\aadi2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [07:42:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n",
      "2025-06-04 07:42:57,461 - INFO - XGBoost saved to saved_models\\XGBoost.model with Test MSE: 1.7114\n",
      "Models:  33%|███▎      | 1/3 [00:02<00:05,  2.64s/model]2025-06-04 07:42:57,462 - INFO - Training LightGBM...\n",
      "c:\\Users\\aadi2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\aadi2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\aadi2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 503, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\aadi2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 971, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\aadi2\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py\", line 1456, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49663\n",
      "[LightGBM] [Info] Number of data points in the train set: 119, number of used features: 24730\n",
      "[LightGBM] [Info] Start training from score 4.115210\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 07:42:59,772 - ERROR - Error training LightGBM: 'LGBMRegressor' object has no attribute 'save_model'\n",
      "Models:  67%|██████▋   | 2/3 [00:04<00:02,  2.45s/model]2025-06-04 07:42:59,773 - INFO - Training CatBoost...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 07:43:14,467 - INFO - CatBoost saved to saved_models\\CatBoost.model with Test MSE: 1.1173\n",
      "Models: 100%|██████████| 3/3 [00:19<00:00,  6.55s/model]\n"
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "model_save_dir = 'saved_models'\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "results = {}\n",
    "\n",
    "logger.info(f\"Starting training for {len(models)} models...\")\n",
    "\n",
    "for name, model in tqdm(models.items(), desc=\"Models\", unit=\"model\"):\n",
    "    logger.info(f\"Training {name}...\")\n",
    "    try:\n",
    "        model.fit(X_train_filtered, y_train)\n",
    "        y_pred = model.predict(X_test_filtered)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        results[name] = mse\n",
    "\n",
    "        model_path = os.path.join(model_save_dir, f\"{name}.model\")\n",
    "        if name == 'CatBoost':\n",
    "            model.save_model(model_path, format='cbm')\n",
    "        else:\n",
    "            model.save_model(model_path)\n",
    "\n",
    "        logger.info(f\"{name} saved to {model_path} with Test MSE: {mse:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training {name}: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01462456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
